{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": "import requests, zipfile, io\nfrom datasets import load_dataset\n\nfrom torch.utils.data import Dataset\nimport os\nfrom PIL import Image\n\nclass SemanticSegmentationDataset(Dataset):\n    \"\"\"Image (semantic) segmentation dataset.\"\"\"\n\n    def __init__(self, root_dir, feature_extractor, train=True):\n        \"\"\"\n        Args:\n            root_dir (string): Root directory of the dataset containing the images + annotations.\n            feature_extractor (SegFormerFeatureExtractor): feature extractor to prepare images + segmentation maps.\n            train (bool): Whether to load \"training\" or \"validation\" images + annotations.\n        \"\"\"\n        self.root_dir = root_dir\n        self.feature_extractor = feature_extractor\n        self.train = train\n\n        sub_path = \"train\" if self.train else \"validation\"\n        self.img_dir = os.path.join(self.root_dir, sub_path, \"rgb\")\n        self.ann_dir = os.path.join(self.root_dir, sub_path, \"labels\")\n        \n        # read images\n        image_file_names = []\n        for root, dirs, files in os.walk(self.img_dir):\n          image_file_names.extend(files)\n        self.images = sorted(image_file_names)\n        \n        # read annotations\n        annotation_file_names = []\n        for root, dirs, files in os.walk(self.ann_dir):\n          annotation_file_names.extend(files)\n        self.annotations = sorted(annotation_file_names)\n\n        assert len(self.images) == len(self.annotations), \"There must be as many images as there are segmentation maps\"\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        \n        image = Image.open(os.path.join(self.img_dir, self.images[idx]))\n        segmentation_map = Image.open(os.path.join(self.ann_dir, self.annotations[idx]))\n\n        # randomly crop + pad both image and segmentation map to same size\n        encoded_inputs = self.feature_extractor(image, segmentation_map, return_tensors=\"pt\")\n\n        for k,v in encoded_inputs.items():\n          encoded_inputs[k].squeeze_() # remove batch dimension\n\n        return encoded_inputs\n    \n    \n"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import SegformerFeatureExtractor\n\nroot_dir = '/home/klimenko/seg_materials/VAL_SEGFORMER/data/4/'# '/home/klimenko/facade_materials/materials/'\nfeature_extractor = SegformerFeatureExtractor(reduce_labels=True)\n\ntrain_dataset = SemanticSegmentationDataset(root_dir=root_dir, feature_extractor=feature_extractor)\nvalid_dataset = SemanticSegmentationDataset(root_dir=root_dir, feature_extractor=feature_extractor, train=False)"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Number of training examples: 121\nNumber of validation examples: 23\n"
    }
   ],
   "source": "print(\"Number of training examples:\", len(train_dataset))\nprint(\"Number of validation examples:\", len(valid_dataset))"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Some weights of the model checkpoint at nvidia/mit-b0 were not used when initializing SegformerForSemanticSegmentation: ['classifier.weight', 'classifier.bias']\n- This IS expected if you are initializing SegformerForSemanticSegmentation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing SegformerForSemanticSegmentation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b0 and are newly initialized: ['decode_head.linear_c.3.proj.weight', 'decode_head.linear_c.2.proj.weight', 'decode_head.batch_norm.bias', 'decode_head.batch_norm.running_mean', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.3.proj.bias', 'decode_head.batch_norm.weight', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.classifier.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.linear_fuse.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.batch_norm.running_var', 'decode_head.linear_c.0.proj.weight', 'decode_head.classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
    }
   ],
   "source": "from torch.utils.data import DataLoader\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=10, shuffle=True)\nvalid_dataloader = DataLoader(valid_dataset, batch_size=1)\n\nfrom transformers import SegformerForSemanticSegmentation\nimport json\nfrom huggingface_hub import cached_download, hf_hub_url, hf_hub_download\n\n# load id2label mapping from a JSON on the hub\nid2label = json.load(open('materials.json'))\nid2label = {int(k): v for k, v in id2label.items()}\nlabel2id = {v: k for k, v in id2label.items()}\n\n# define model\nmodel = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/mit-b0\",\n                                                         num_labels=7, \n                                                         id2label=id2label, \n                                                         label2id=label2id,\n)\n"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "torch.Size([1, 7, 1000, 1000])\ntorch.Size([1, 7, 921, 921])\ntorch.Size([1, 7, 921, 921])\ntorch.Size([1, 7, 921, 921])\ntorch.Size([1, 7, 921, 921])\ntorch.Size([1, 7, 921, 921])\ntorch.Size([1, 7, 921, 921])\ntorch.Size([1, 7, 921, 921])\ntorch.Size([1, 7, 921, 921])\ntorch.Size([1, 7, 921, 921])\ntorch.Size([1, 7, 921, 921])\ntorch.Size([1, 7, 921, 921])\ntorch.Size([1, 7, 1000, 1000])\ntorch.Size([1, 7, 1000, 1000])\ntorch.Size([1, 7, 921, 921])\ntorch.Size([1, 7, 921, 921])\ntorch.Size([1, 7, 921, 921])\ntorch.Size([1, 7, 1000, 1000])\ntorch.Size([1, 7, 1000, 1000])\ntorch.Size([1, 7, 921, 921])\ntorch.Size([1, 7, 921, 921])\ntorch.Size([1, 7, 2304, 3066])\ntorch.Size([1, 7, 921, 921])\n"
    }
   ],
   "source": "import torch\nimport numpy as np\nfrom torch import nn\nfrom sklearn.metrics import accuracy_score\nfrom tqdm.notebook import tqdm\n\nmodel = SegformerForSemanticSegmentation.from_pretrained(\"weights/fold_4_10_ep_0.91.pth\")\n\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nfor filename in os.listdir(root_dir+'validation/rgb/'):\n    \n    image_path = root_dir+'validation/rgb/'+filename\n    image = Image.open(image_path).convert(\"RGB\")\n\n    inputs = feature_extractor(images=image, return_tensors=\"pt\").to(device)\n    resized_img = image.resize((128, 128))\n    image_np = np.array(resized_img)\n    \n    model.eval()\n    with torch.no_grad():\n        outputs = model(inputs.pixel_values).logits\n        upsampled_logits = nn.functional.interpolate(outputs,size=image.size[::-1],mode='bilinear',align_corners=False)\n        print(upsampled_logits.shape)\n        seg = upsampled_logits.cpu().argmax(dim=1)[0].numpy()\n        replacement_dict = {0: 10, 1: 11, 2:12, 3:13, 4:16, 5:17, 6:0}\n        seg2 = np.vectorize(replacement_dict.get)(seg)\n        seg3 = np.stack([seg2] * 3, axis=-1)\n        \n        np.save('/home/klimenko/seg_materials/VAL_SEGFORMER/results/'+filename.replace(\".png\", \".npy\"), seg2)\n        "
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "facade",
   "language": "python",
   "name": "facade"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
